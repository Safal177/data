{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71a4c1-9dc1-43fd-bb95-f913fb514355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d065f2b-f680-4209-a816-b82dc7cc66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My project worked on the dataset “train.csv” and “test.csv” obtained from the MS AI Machine Learning using Python with Python program with\n",
    "Simplilearn. I received permission to work on this dataset and to upload my work on Python code and results to my GitHub account. The goal of \n",
    "this analysis is to show my Python coding skills with results using dataset associated with (Mercedes- Benz Greener Manufacturing) project.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d32003-221e-40d8-937a-86328aee2bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76001cab-41b8-478c-ae17-39e825f05a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warning note\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e4023b-ee55-4604-a514-d6b9f599fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID       y  X0 X1  X2 X3 X4  X5 X6 X8  ...  X375  X376  X377  X378  \\\n",
      "4204  8405  107.39  ak  s  as  c  d  aa  d  q  ...     1     0     0     0   \n",
      "4205  8406  108.77   j  o   t  d  d  aa  h  h  ...     0     1     0     0   \n",
      "4206  8412  109.22  ak  v   r  a  d  aa  g  e  ...     0     0     1     0   \n",
      "4207  8415   87.48  al  r   e  f  d  aa  l  u  ...     0     0     0     0   \n",
      "4208  8417  110.85   z  r  ae  c  d  aa  g  w  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 378 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Look at data structure\n",
    "print(df_train.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db0a468-8ce1-4933-92f6-9c99fcc5bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  X0  X1  X2 X3 X4  X5 X6 X8  X10  ...  X375  X376  X377  X378  \\\n",
      "4204  8410  aj   h  as  f  d  aa  j  e    0  ...     0     0     0     0   \n",
      "4205  8411   t  aa  ai  d  d  aa  j  y    0  ...     0     1     0     0   \n",
      "4206  8413   y   v  as  f  d  aa  d  w    0  ...     0     0     0     0   \n",
      "4207  8414  ak   v  as  a  d  aa  c  q    0  ...     0     0     1     0   \n",
      "4208  8416   t  aa  ai  c  d  aa  g  r    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 377 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Look at data structure\n",
    "print(df_test.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb9c6ad-cb58-4a10-b484-1eed5c9cf2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c5f4f4d-c3af-4994-a9c5-bd64b98ec3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID       y  X0 X1  X2 X3 X4  X5 X6 X8  ...  X375  X376  X377  X378  \\\n",
      "4207  8415   87.48  al  r   e  f  d  aa  l  u  ...     0     0     0     0   \n",
      "4208  8417  110.85   z  r  ae  c  d  aa  g  w  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[2 rows x 378 columns]\n",
      "\n",
      "\n",
      "shape of train data:\n",
      " (4209, 378)\n",
      "\n",
      "\n",
      "        ID  X0  X1  X2 X3 X4  X5 X6 X8  X10  ...  X375  X376  X377  X378  \\\n",
      "4207  8414  ak   v  as  a  d  aa  c  q    0  ...     0     0     1     0   \n",
      "4208  8416   t  aa  ai  c  d  aa  g  r    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[2 rows x 377 columns]\n",
      "\n",
      "\n",
      "shape of test data:\n",
      " (4209, 377)\n",
      "\n",
      "\n",
      " list of zero varience numeric columns:\n",
      " ['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347']\n",
      "\n",
      "\n",
      "names of  categorical columns:\n",
      " Index(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'], dtype='object')\n",
      "\n",
      "\n",
      " list of zero varience columns (numeric and categorical columns):\n",
      " ['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "missing numbers in each column:\n",
      " ID      0\n",
      "y       0\n",
      "X0      0\n",
      "X1      0\n",
      "X2      0\n",
      "       ..\n",
      "X380    0\n",
      "X382    0\n",
      "X383    0\n",
      "X384    0\n",
      "X385    0\n",
      "Length: 366, dtype: int64\n",
      "\n",
      "\n",
      "columns with missing value >0 :\n",
      " Series([], dtype: int64)\n",
      " test columns with missing value > :\n",
      " Series([], dtype: int64)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " number of categorical columns in the train dataset:\n",
      " 8\n",
      "\n",
      "\n",
      " XGBoost is usable.\n",
      "root mean squared error under cross validation:\n",
      " [ 9.38366633 11.63855659  9.16742458  8.27652226  8.94010625  9.08893116]\n",
      "mean root mean squared error under cross validation:\n",
      " 9.415867862315933\n",
      "std in  root mean squared error under cross validation:\n",
      " 1.0515426218099082\n",
      " 8 predictated data rows:\n",
      "    ID           y\n",
      "0   1  130.752487\n",
      "1   2   88.299248\n",
      "2   3   76.056564\n",
      "3   4   80.731667\n",
      "4   5   78.331276\n",
      "5   8   92.340950\n",
      "6  10  127.759995\n",
      "7  11   93.061668\n"
     ]
    }
   ],
   "source": [
    "# Define  Mercedes_Benz_Greener_Manufacturing_pipeline function\n",
    "def Mercedes_Benz_Greener_Manufacturing_pipeline(train_data=\"train.csv\", test_data = \"test.csv\", predictations_data = \"predictations_data.csv\"):\n",
    "    # read train data\n",
    "    df_train_data = pd.read_csv(train_data)\n",
    "    print(df_train_data.tail(2))\n",
    "    print(\"\\n\")\n",
    "    print(\"shape of train data:\\n\", df_train_data.shape)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # retrieve test data\n",
    "    df_test_data = pd.read_csv(test_data)\n",
    "    print(df_test_data.tail(2))\n",
    "    print(\"\\n\")\n",
    "    print(\"shape of test data:\\n\", df_test_data.shape)\n",
    "\n",
    "    # Remove variable(s) having zero variance\n",
    "    columns_zero_varience = []   # collect zero variance features names\n",
    "    \n",
    "    # collect the names of numeric columns\n",
    "    names_num = df_train_data.select_dtypes(include =[np.number]).columns\n",
    "    #print( names_num)\n",
    "    print(\"\\n\")\n",
    "    #print( names_num.shape)\n",
    "    numeric_columns_list = names_num.tolist()\n",
    "    #print(numeric_columns_list)\n",
    "\n",
    "\n",
    "    # problem # 1: If for any column(s), the variance is equal to zero, then we need to remove those variable(s).\n",
    "    \n",
    "    # collect numeric columns if they have zero variance\n",
    "\n",
    "    for i in numeric_columns_list:\n",
    "        if df_train_data[i].var() == 0:\n",
    "            columns_zero_varience.append(i)   # put numeric columns with zero variance into the columns_zero_varience\n",
    "    print(\" list of zero varience numeric columns:\\n\",  columns_zero_varience)        \n",
    "    print(\"\\n\") \n",
    "\n",
    "    # collect the names of  categorical columns\n",
    "    names_cat = df_train_data.select_dtypes(include =[\"object\"]).columns\n",
    "    print(\"names of  categorical columns:\\n\", names_cat)\n",
    "    print(\"\\n\")\n",
    "    #print( names_cat.shape)\n",
    "    categorical_columns_list = names_cat.tolist()\n",
    "    #print(\"list of categorical columns:\\n\", categorical_columns_list)\n",
    "    \n",
    "    # collect categorical columns if they have zero variance\n",
    "    for j in categorical_columns_list:\n",
    "        if df_train_data[j].nunique() <= 1:\n",
    "            columns_zero_varience.append(j)\n",
    "\n",
    "    print(\" list of zero varience columns (numeric and categorical columns):\\n\",  columns_zero_varience)\n",
    "    \n",
    "    # check\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #  check unique values in the categorical columns\n",
    "    #print(df_train_data[categorical_columns_list].nunique().sort_values())\n",
    "\n",
    "    # drop duplicate columns using the set function\n",
    "    columns_zero_varience = sorted(list(set(columns_zero_varience)))\n",
    "    #print(\"drop duplicate columns using the set function:\\n\", (columns_zero_varience))\n",
    "    \n",
    "    # Eliminate \"columns_zero_varience\" from the train dataset\n",
    "    df_train_data.drop(columns = columns_zero_varience, errors = \"ignore\", inplace = True)\n",
    "    #print(df_train_data.shape)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Filter out \"columns_zero_varience\" from the test dataset\n",
    "    df_test_data.drop(columns = columns_zero_varience, errors = \"ignore\", inplace = True)\n",
    "    # print(df_test_data.shape)\n",
    "\n",
    "    # problem # 2:  check for null and unique values for test and train sets.\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Look at the train dataset\n",
    "    \n",
    "    print(\"missing numbers in each column:\\n\", df_train_data.isnull().sum())      # missing numbers in each column\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # listed columns with missing value > 0 in the train data\n",
    "    print(\"columns with missing value >0 :\\n\", df_train_data.isnull().sum()[df_train_data.isnull().sum()>0]) \n",
    "\n",
    "    # Look at the test dataset\n",
    "    \n",
    "    # listed columns with missing value > 0  in the test data\n",
    "    print(\" test columns with missing value > :\\n\", df_test_data.isnull().sum()[df_test_data.isnull().sum()>0]) \n",
    "\n",
    "    # Problem # 3: Apply label encoder\n",
    "\n",
    "    # assign the label column from the train dataset. We need to predict this column\n",
    "    label_train =  \"y\"\n",
    "    \n",
    "    # set the ID column which is common in both train and test dataset\n",
    "    ID_train_test = \"ID\"\n",
    "    \n",
    "    # save ID column into df_train_data\n",
    "    df_train_data_ID = df_train_data[ID_train_test].copy()   # pull ID\n",
    "\n",
    "    # save ID column into df_train_data\n",
    "    df_test_data_ID = df_test_data[ID_train_test].copy()   # pull ID\n",
    "\n",
    "    \n",
    "    # check\n",
    "    #print(\" new ID column name:\\n\",  df_train_data_ID)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # take the label column from th train dataset and save it as label_y. \n",
    "    label_y = df_train_data[label_train].copy()\n",
    "    #print(label_y)\n",
    "\n",
    "    # strip out ID and label from train dataset\n",
    "    \n",
    "    df_train_data = df_train_data.drop(columns = [ID_train_test, label_train])\n",
    "    #print(\" strip out ID and label from train dataset:\\n\",  df_train_data)\n",
    "\n",
    "    # delete the ID column from the test dataset\n",
    "    df_test_data =  df_test_data.drop(columns = [ID_train_test])\n",
    "    # print(\" omit out ID from test dataset:\\n\",  df_test_data)\n",
    "    \n",
    "    # for encoding, find categorical columns in the training dataset\n",
    "    categorical_columns =  df_train_data.select_dtypes (include = [\"object\"]).columns.tolist()\n",
    "    #print(\" categorical columns from the train dataset:\\n\", categorical_columns)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\" number of categorical columns in the train dataset:\\n\", len(categorical_columns))\n",
    "\n",
    "\n",
    "    # encode the categorical columns if needed\n",
    "    if len(categorical_columns) > 0:\n",
    "        # join the train and test datasets with axis = 0\n",
    "        train_test_cat_join = pd.concat([df_train_data[categorical_columns], df_test_data[categorical_columns]], axis = 0).astype(str)\n",
    "        \n",
    "        # record all categorical encoders\n",
    "        categorical_encoders = {}\n",
    "        # use for loop\n",
    "        for i in categorical_columns:\n",
    "            # define the LabelEncoder() \n",
    "            label_encoder =  LabelEncoder()\n",
    "            # fit the LabelEncoder() to the categorical columns from train and test datasets. \n",
    "            label_encoder.fit(train_test_cat_join[i].values)\n",
    "            \n",
    "            # change the categorical column in training dataset into numerical labels \n",
    "            df_train_data[i] = label_encoder.transform(df_train_data[i].astype(str).values)\n",
    "            \n",
    "            # change the categorical column in testing dataset into numerical labels \n",
    "            df_test_data[i] = label_encoder.transform(df_test_data[i].astype(str).values)\n",
    "            \n",
    "        # display the updated training dataset\n",
    "        #print(\"updated training datase[df_train_data]t:\\n\",  df_train_data)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # display the updated testing dataset\n",
    "        #print(\"updated training datase[df_test_data]t:\\n\",  df_test_data)\n",
    "            \n",
    "    # process copies of the dataset to keep the originals  \n",
    "    df_train_data_X = df_train_data.copy()\n",
    "\n",
    "    df_test_data_X = df_test_data.copy()\n",
    "\n",
    "    # Verify that train and test datasets have the same features\n",
    "    if set( df_train_data_X) != set(df_test_data_X.columns):\n",
    "        df_test_data_X = df_test_data_X.reindex(columns = df_train_data_X.columns, fill_value = 0)\n",
    "    #print(df_train_data_X.shape[1])   # count number of columns\n",
    "\n",
    "    \n",
    "    # scale both train and test dataset  \n",
    " \n",
    "    standard_scaler = StandardScaler()\n",
    "\n",
    "    scaled_train_data = standard_scaler.fit_transform(df_train_data)\n",
    "\n",
    "    scaled_test_data = standard_scaler.transform(df_test_data)\n",
    "\n",
    "\n",
    "   \n",
    "    # Perform dimensionality reduction.\n",
    "\n",
    "    dimension_reduction_pca = PCA(n_components = 0.92, svd_solver = \"full\", random_state = 42)  # set up PCA\n",
    "\n",
    "    pca_scaled_train_data = dimension_reduction_pca.fit_transform(scaled_train_data)\n",
    "\n",
    "    pca_scaled_test_data = dimension_reduction_pca.transform(scaled_test_data)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Predict your test_df values using XGBoost.\n",
    "\n",
    "    # check model (XGBoost) if it is usable.\n",
    "\n",
    "    xgboost_usable = False   # No xgboost status\n",
    "\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        from xgboost import XGBRegressor\n",
    "        xgboost_usable = True   # if xgboost is available, xgboost_usable becomes True. \n",
    "        print(\" XGBoost is usable.\")\n",
    "    except Exception as exc:\n",
    "        print(\"XGBoost is nit usable. We need to use GradientBoostingRegressor \\n\", exc )\n",
    "        \n",
    "      \n",
    "    if xgboost_usable:  # if true, move fprward\n",
    "        model = XGBRegressor( n_estimators = 450, learning_rate = 0.04, max_depth = 7, verbosity = 0, subsample = 0.7, gamma = 0.15, random_state = 42)\n",
    "\n",
    "    else:\n",
    "        model =  GradientBoostingRegressor(n_estimators = 450, learning_rate = 0.04, subsample = 0.7, loss = \"squared_error\", max_depth = 7, random_state = 42)      \n",
    "        \n",
    "\n",
    "    # set up a 6-fold cross-validation \n",
    "    k_fold = KFold(n_splits = 6, shuffle = True, random_state = 42)\n",
    "\n",
    "    cross_validation_neg_mse = cross_val_score(model, pca_scaled_train_data, label_y, cv = k_fold,\n",
    "                n_jobs = 1, error_score = \"raise\", scoring = \"neg_mean_squared_error\", verbose = 1)\n",
    "    \n",
    "    # find root mean squared error \n",
    "    cross_validation_root_mean_squared_error = np.sqrt(-cross_validation_neg_mse)\n",
    "    \n",
    "    print(\"root mean squared error under cross validation:\\n\", cross_validation_root_mean_squared_error )\n",
    "\n",
    "    print(\"mean root mean squared error under cross validation:\\n\", cross_validation_root_mean_squared_error.mean() )\n",
    "\n",
    "    print(\"std in  root mean squared error under cross validation:\\n\", cross_validation_root_mean_squared_error.std() )\n",
    "\n",
    "    # train the model with the training dataset\n",
    "    model.fit(pca_scaled_train_data, label_y)\n",
    "\n",
    "    # predict the target values\n",
    "    predicted_y = model.predict(pca_scaled_train_data)\n",
    "\n",
    "    # make a dataframe using IDs, and prediction\n",
    "\n",
    "    predict_data_y = pd.DataFrame({ ID_train_test:  df_test_data_ID,  label_train : predicted_y}\n",
    "                                   )\n",
    "    # save predictated data\n",
    "    predict_data_y.to_csv(predictations_data, index = False)\n",
    "\n",
    "    # predictations_data\n",
    "    print(\" 8 predictated data rows:\\n\", predict_data_y.head(8))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Mercedes_Benz_Greener_Manufacturing_pipeline()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c8bc2-5c4b-45bd-8295-2b6dc8050d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
